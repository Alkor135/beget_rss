"""
–°–∫—Ä–∏–ø—Ç –∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–Ω–µ–≤–Ω—ã–µ –∫–æ—Ç–∏—Ä–æ–≤–∫–∏ —Ñ—å—é—á–µ—Ä—Å–æ–≤ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –ë–î –∏ –∫—ç—à–∞, –æ–±—ä–µ–¥–∏–Ω—è—è –∏—Ö –ø–æ –¥–∞—Ç–µ.
–î–ª—è –∫–∞–∂–¥–æ–π –¥–∞—Ç—ã –Ω–∞—á–∏–Ω–∞—è —Å –∑–∞–¥–∞–Ω–Ω–æ–π —Ç–æ—á–∫–∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–∏ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ k –≤–µ–∫—Ç–æ—Ä–∞–º–∏
(–æ—Ç 3 –¥–æ 30) —á–µ—Ä–µ–∑ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ.
–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–π –¥–µ–Ω—å –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã (–ø–æ NEXT_BODY) –≤ —Ç–µ–∫—É—â–µ–º –∏
–Ω–∞–π–¥–µ–Ω–Ω–æ–º –ø—Ä–æ—à–ª–æ–º –¥–Ω–µ.
–ï—Å–ª–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–æ–≤–ø–∞–¥–∞—é—Ç, –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –º–æ–¥—É–ª—å –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏, –∏–Ω–∞—á–µ ‚Äî –º–∏–Ω—É—Å –µ—ë –º–æ–¥—É–ª—å (–æ—à–∏–±–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞).
–°—á–∏—Ç–∞–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π P/L –∑–∞ test_days –¥–Ω–µ–π –≤–ø–µ—Ä—ë–¥ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ k –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–µ–µ –æ–∫–Ω–æ (MAX_k) –ø–æ
–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–º—É P/L.
–§–æ—Ä–º–∏—Ä—É–µ—Ç —Å–∏–≥–Ω–∞–ª: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ MAX_k –∏–∑ –ª—É—á—à–µ–≥–æ –æ–∫–Ω–∞ –∫–∞–∫ –ø—Ä–æ–≥–Ω–æ–∑ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –¥–µ–Ω—å.
–í—ã–≤–æ–¥–∏—Ç –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π P/L –ø–æ —ç—Ç–∏–º —Å–∏–≥–Ω–∞–ª–∞–º –∏ —Å—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ–∏–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.
"""

from pathlib import Path
from datetime import datetime
import pickle
import sqlite3
import logging
import yaml
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# –ü—É—Ç—å –∫ settings.yaml –≤ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, —á—Ç–æ –∏ —Å–∫—Ä–∏–ø—Ç
SETTINGS_FILE = Path(__file__).parent / "settings.yaml"

# –ß—Ç–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫
with open(SETTINGS_FILE, 'r', encoding='utf-8') as f:
    settings = yaml.safe_load(f)

# ==== –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ====
ticker = settings['ticker']
ticker_lc = ticker.lower()
cache_file = Path(settings['cache_file'].replace('{ticker_lc}', ticker_lc))  # –ü—É—Ç—å –∫ pkl-—Ñ–∞–π–ª—É —Å –∫—ç—à–µ–º
path_db_day = Path(settings['path_db_day'].replace('{ticker}', ticker))  # –ü—É—Ç—å –∫ –ë–î –¥–Ω–µ–≤–Ω—ã—Ö –∫–æ—Ç–∏—Ä–æ–≤–æ–∫
min_prev_files = settings.get('min_prev_files', 2)
test_days = settings.get('test_days', 23) + 1
START_DATE = settings.get('start_date', "2025-10-01")
# START_DT = datetime.strptime(START_DATE, "%Y-%m-%d").date()

# === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ===
log_dir = Path(__file__).parent / 'log'
log_dir.mkdir(parents=True, exist_ok=True)
# –ò–º—è —Ñ–∞–π–ª–∞ –ª–æ–≥–∞ —Å –¥–∞—Ç–æ–π –∏ –≤—Ä–µ–º–µ–Ω–µ–º –∑–∞–ø—É—Å–∫–∞ (–æ–¥–∏–Ω —Ñ–∞–π–ª –Ω–∞ –∑–∞–ø—É—Å–∫!)
timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
log_file = log_dir / f'simulate_trade_{timestamp}.txt'

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: –¢–û–õ–¨–ö–û –æ–¥–∏–Ω —Ñ–∞–π–ª + –∫–æ–Ω—Å–æ–ª—å
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),  # –æ–¥–∏–Ω —Ñ–∞–π–ª
        logging.StreamHandler()                           # –∫–æ–Ω—Å–æ–ª—å
    ]
)

# –†—É—á–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤ (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ 3 —Å–∞–º—ã—Ö –Ω–æ–≤—ã—Ö)
def cleanup_old_logs(log_dir: Path, max_files: int = 3):
    """–£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –ª–æ–≥-—Ñ–∞–π–ª—ã, –æ—Å—Ç–∞–≤–ª—è—è max_files —Å–∞–º—ã—Ö –Ω–æ–≤—ã—Ö."""
    log_files = sorted(log_dir.glob("simulate_trade_*.txt"))
    if len(log_files) > max_files:
        for old_file in log_files[:-max_files]:
            try:
                old_file.unlink()
                print(f"–£–¥–∞–ª—ë–Ω —Å—Ç–∞—Ä—ã–π –ª–æ–≥: {old_file.name}")
            except Exception as e:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {old_file}: {e}")

# –í—ã–∑—ã–≤–∞–µ–º –æ—á–∏—Å—Ç–∫—É –ü–ï–†–ï–î –Ω–∞—á–∞–ª–æ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
cleanup_old_logs(log_dir, max_files=3)
logging.info(f"üöÄ –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞. –õ–æ–≥-—Ñ–∞–π–ª: {log_file}")

def load_quotes(path_db_quote):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ç–∏—Ä–æ–≤–æ–∫ –∏ —Ä–∞—Å—á–µ—Ç NEXT_BODY."""
    with sqlite3.connect(path_db_quote) as conn:
        df = pd.read_sql_query(
            "SELECT TRADEDATE, OPEN, CLOSE FROM Futures",
            conn,
            parse_dates=['TRADEDATE']  # <-- –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º TRADEDATE –≤ datetime
        )
    df = df.set_index('TRADEDATE').sort_index()
    df['NEXT_BODY'] = (df['CLOSE'] - df['OPEN']).shift(-1)
    df = df.dropna(subset=['NEXT_BODY'])
    return df[['NEXT_BODY']]

def load_cache(cache_file_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."""
    with open(cache_file_path, 'rb') as f:
        df = pickle.load(f)
    df['TRADEDATE'] = pd.to_datetime(df['TRADEDATE'])
    return df.set_index('TRADEDATE').sort_index()

def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É"""
    denom = (np.linalg.norm(a) * np.linalg.norm(b))
    if denom == 0:
        return 0.0
    return float(np.dot(a, b) / denom)

def compute_max_k(
    df: pd.DataFrame,
    start_date: pd.Timestamp,
    k: int,
    col_vectors: str = "VECTORS",
    col_body: str = "NEXT_BODY"
) -> pd.Series:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç Series –¥–ª—è –∫–æ–ª–æ–Ω–∫–∏ MAX_k
    """
    result = pd.Series(index=df.index, dtype=float)

    dates = df.index
    start_pos = dates.get_loc(start_date)

    for i in range(start_pos, len(df)):
        if i < k:
            continue

        vec_cur = df.iloc[i][col_vectors]
        body_cur = df.iloc[i][col_body]

        similarities = []
        indices = []

        for j in range(i - k, i):
            vec_prev = df.iloc[j][col_vectors]
            sim = cosine_similarity(vec_cur, vec_prev)
            similarities.append(sim)
            indices.append(j)

        # –∏–Ω–¥–µ–∫—Å —Å–∞–º–æ–π –ø–æ—Ö–æ–∂–µ–π —Å—Ç—Ä–æ–∫–∏
        best_j = indices[int(np.argmax(similarities))]
        body_prev = df.iloc[best_j][col_body]

        if np.sign(body_cur) == np.sign(body_prev):
            result.iloc[i] = abs(body_cur)
        else:
            result.iloc[i] = -abs(body_cur)

    return result

def main(path_db_day, cache_file):
    df_bar = load_quotes(path_db_day)  # –ó–∞–≥—Ä—É–∑–∫–∞ DF —Å –¥–Ω–µ–≤–Ω—ã–º–∏ –∫–æ—Ç–∏—Ä–æ–≤–∫–∞–º–∏ (—Å 21:00 –ø—Ä–µ–¥. —Å–µ—Å—Å–∏–∏)
    df_emb = load_cache(cache_file)  # –ó–∞–≥—Ä—É–∑–∫–∞ DF —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –Ω–æ–≤–æ—Å—Ç–µ–π

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤ –ø–æ –∏–Ω–¥–µ–∫—Å—É TRADEDATE
    df_combined = df_bar.join(df_emb[['VECTORS']], how='inner')  # 'inner' ‚Äî —Ç–æ–ª—å–∫–æ –æ–±—â–∏–µ –¥–∞—Ç—ã

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–ª–æ–Ω–æ–∫ MAX_3 ‚Ä¶ MAX_30
    start_date = pd.to_datetime(START_DATE)
    for k in range(3, 31):
        col_name = f"MAX_{k}"
        logging.info(f"üìä –†–∞—Å—á—ë—Ç {col_name}")
        df_combined[col_name] = compute_max_k(
            df=df_combined,
            start_date=start_date,
            k=k
        )

    # === –ó–∞–º–µ–Ω–∞ NaN –Ω–∞ 0.0 –≤–æ –≤—Å–µ—Ö MAX_ –∫–æ–ª–æ–Ω–∫–∞—Ö ===
    max_cols = [f"MAX_{k}" for k in range(3, 31)]
    df_combined[max_cols] = df_combined[max_cols].fillna(0.0)

    # === –†–∞—Å—á—ë—Ç PL_ –∫–æ–ª–æ–Ω–æ–∫ ===
    for k in range(3, 31):
        max_col = f"MAX_{k}"
        pl_col = f"PL_{k}"

        df_combined[pl_col] = (
            df_combined[max_col]
            .shift(1)  # –∏—Å–∫–ª—é—á–∞–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–æ–∫—É
            .rolling(window=test_days, min_periods=1)
            .sum()
        )

    # –û—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥
    with pd.option_context(
        "display.width", 1000,
        "display.max_columns", 10,
        "display.max_colwidth", 120
    ):
        print(df_bar)
        print(df_emb)
        print(df_combined[["NEXT_BODY", "VECTORS"]])
        print(df_combined)

    # === –ó–∞–º–µ–Ω–∞ NaN –Ω–∞ 0.0 –≤–æ –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö ===
    df_combined = df_combined.fillna(0.0)

    # === –û–°–¢–ê–í–ò–¢–¨ –¢–û–õ–¨–ö–û –ù–£–ñ–ù–´–ï –ö–û–õ–û–ù–ö–ò ===
    final_cols = [f"MAX_{k}" for k in range(3, 31)] + [f"PL_{k}" for k in range(3, 31)]
    df_combined = df_combined[final_cols].copy()

    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∏–Ω–¥–µ–∫—Å—É (–ø–æ –¥–∞—Ç–µ)
    df_combined.sort_index(inplace=True)

    # –û—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥
    with pd.option_context(
        "display.width", 1000,
        "display.max_columns", 24,
        "display.max_colwidth", 120,
        "display.min_rows", 30
    ):
        print(df_combined[[f"PL_{k}" for k in range(3, 31)]])

    # ===============================
    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ df_rez
    # ===============================

    pl_cols = [f"PL_{k}" for k in range(3, 31)]
    max_cols = [f"MAX_{k}" for k in range(3, 31)]

    rows = []

    for idx, row in df_combined.iterrows():
        trade_date = idx

        # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å—Ä–µ–¥–∏ PL_3 ... PL_30
        pl_values = row[pl_cols]
        pl_max = pl_values.max()

        pl_result = 0.0

        if pl_max > 0.0:
            # –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º PL
            best_pl_col = pl_values.idxmax()  # –Ω–∞–ø—Ä–∏–º–µ—Ä "PL_7"
            n = int(best_pl_col.split("_")[1])  # -> 7

            # —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è –∫–æ–ª–æ–Ω–∫–∞ MAX_n
            max_col = f"MAX_{n}"
            pl_result = row[max_col]

        rows.append({
            "TRADEDATE": trade_date,
            "P/L": pl_result
        })

    df_rez = pd.DataFrame(rows).set_index("TRADEDATE")

    # ===============================
    # –í—ã–≤–æ–¥ df_rez –≤ –∫–æ–Ω—Å–æ–ª—å
    # ===============================
    with pd.option_context(
            "display.width", 1000,
            "display.max_columns", 10,
            "display.max_colwidth", 120
    ):
        print(df_rez)

    # ===============================
    # –ì—Ä–∞—Ñ–∏–∫ cumulative P/L
    # ===============================
    df_rez["CUM_P/L"] = df_rez["P/L"].cumsum()

    plt.figure(figsize=(12, 6))
    plt.plot(df_rez.index, df_rez["CUM_P/L"])
    plt.title("Cumulative P/L")
    plt.xlabel("Date")
    plt.ylabel("P/L")
    plt.grid(True)
    plt.tight_layout()
    # plt.show()

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
    plot_dir = Path(__file__).parent / 'plots'
    plot_dir.mkdir(exist_ok=True)
    plot_path = plot_dir / f'cumulative_pl_{timestamp}.png'
    plt.savefig(plot_path)
    logging.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {plot_path}")
    plt.close()  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å

if __name__ == "__main__":
    main(path_db_day, cache_file)