"""
–°–∫—Ä–∏–ø—Ç —Å–æ–∑–¥–∞—ë—Ç –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω—ã—Ö markdown-–æ—Ç—á—ë—Ç–æ–≤.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Ollama (–ª–æ–∫–∞–ª—å–Ω–æ) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Ç–µ–∫—Å—Ç–∞.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏ bge-m3 –∏ qwen3-embedding:0.6b —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ —Ç–æ–∫–µ–Ω–∞–º.
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–µ–∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —á–µ—Ä–µ–∑ MD5-—Ö—ç—à–∏ –∏ –±–µ—Ä—ë—Ç –∏—Ö –∏–∑ –∫—ç—à–∞.
–†–∞–±–æ—Ç–∞–µ—Ç –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –∏–ª–∏ –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã.
–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ pickle-—Ñ–∞–π–ª –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –¥—Ä—É–≥–∏—Ö —Å–∫—Ä–∏–ø—Ç–∞—Ö.
–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª —Å —Ä–æ—Ç–∞—Ü–∏–µ–π (–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –ª–æ–≥–∞).
"""

from pathlib import Path
import pickle
import hashlib
import numpy as np
from chromadb.utils.embedding_functions import OllamaEmbeddingFunction
import logging
import yaml
from datetime import datetime
import pandas as pd
import tiktoken
import sys

# –ü—É—Ç—å –∫ settings.yaml –≤ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, —á—Ç–æ –∏ —Å–∫—Ä–∏–ø—Ç
SETTINGS_FILE = Path(__file__).parent / "settings.yaml"

# –ß—Ç–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫
with open(SETTINGS_FILE, 'r', encoding='utf-8') as f:
    settings = yaml.safe_load(f)

# ==== –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ====
ticker = settings['ticker']
ticker_lc = ticker.lower()
url_ai = settings.get('url_ai', 'http://localhost:11434/api/embeddings')  # Ollama API –±–µ–∑ —Ç–∞–π–º-–∞—É—Ç–∞
model_name = settings.get('model_name', 'bge-m3')  # Ollama –º–æ–¥–µ–ª—å
md_path = Path(settings['md_path'])  # –ü—É—Ç—å –∫ markdown-—Ñ–∞–π–ª–∞–º
if model_name == 'bge-m3':
    max_chunk_tokens = 7000  # –î–ª—è bge-m3 (8192 –ª–∏–º–∏—Ç –º–∏–Ω—É—Å –∑–∞–ø–∞—Å)
elif model_name == 'qwen3-embedding:0.6b':
    max_chunk_tokens = 30000  # –î–ª—è qwen3-embedding:0.6b (32768 –ª–∏–º–∏—Ç –º–∏–Ω—É—Å –∑–∞–ø–∞—Å)
elif model_name == 'embeddinggemma':
    max_chunk_tokens = 1600  # –î–ª—è embeddinggemma (2048 –ª–∏–º–∏—Ç –º–∏–Ω—É—Å –∑–∞–ø–∞—Å)
else:
    print('–ü—Ä–æ–≤–µ—Ä—å –º–æ–¥–µ–ª—å')
    sys.exit()

# –ü—É—Ç—å –∫ pkl-—Ñ–∞–π–ª—É —Å –∫—ç—à–µ–º
cache_file = Path(settings['cache_file'].replace('{ticker_lc}', ticker_lc))

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è –ª–æ–≥–æ–≤
log_dir = Path(__file__).parent / 'log'
log_dir.mkdir(parents=True, exist_ok=True)

# –ò–º—è —Ñ–∞–π–ª–∞ –ª–æ–≥–∞ —Å –¥–∞—Ç–æ–π –∏ –≤—Ä–µ–º–µ–Ω–µ–º –∑–∞–ø—É—Å–∫–∞ (–æ–¥–∏–Ω —Ñ–∞–π–ª –Ω–∞ –∑–∞–ø—É—Å–∫!)
timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
log_file = log_dir / f'create_embedding_ollama_{timestamp}.txt'

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: –¢–û–õ–¨–ö–û –æ–¥–∏–Ω —Ñ–∞–π–ª + –∫–æ–Ω—Å–æ–ª—å
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),  # –æ–¥–∏–Ω —Ñ–∞–π–ª
        logging.StreamHandler()                           # –∫–æ–Ω—Å–æ–ª—å
    ]
)

# –†—É—á–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤ (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ 3 —Å–∞–º—ã—Ö –Ω–æ–≤—ã—Ö)
def cleanup_old_logs(log_dir: Path, max_files: int = 3):
    """–£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –ª–æ–≥-—Ñ–∞–π–ª—ã, –æ—Å—Ç–∞–≤–ª—è—è max_files —Å–∞–º—ã—Ö –Ω–æ–≤—ã—Ö."""
    log_files = sorted(log_dir.glob("create_embedding_ollama_*.txt"))
    if len(log_files) > max_files:
        for old_file in log_files[:-max_files]:
            try:
                old_file.unlink()
                print(f"–£–¥–∞–ª—ë–Ω —Å—Ç–∞—Ä—ã–π –ª–æ–≥: {old_file.name}")
            except Exception as e:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {old_file}: {e}")

# –í—ã–∑—ã–≤–∞–µ–º –æ—á–∏—Å—Ç–∫—É –ü–ï–†–ï–î –Ω–∞—á–∞–ª–æ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
cleanup_old_logs(log_dir, max_files=3)
logging.info(f"üöÄ –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞. –õ–æ–≥-—Ñ–∞–π–ª: {log_file}")

enc = tiktoken.get_encoding("cl100k_base")

def token_len(text: str) -> int:
    return len(enc.encode(text))

# === –§—É–Ω–∫—Ü–∏—è –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ Ollama ===
ef = OllamaEmbeddingFunction(model_name=model_name)

def load_existing_cache(cache_file: Path) -> pd.DataFrame | None:
    if cache_file.exists():
        try:
            with open(cache_file, "rb") as f:
                df = pickle.load(f)
            logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫—ç—à: {cache_file}, —Å—Ç—Ä–æ–∫: {len(df)}")
            return df
        except Exception as e:
            logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à {cache_file}: {e}")
    return None

def build_embeddings_df(md_dir: Path, existing_df: pd.DataFrame | None) -> pd.DataFrame:
    """
    –°–æ–∑–¥–∞—ë—Ç –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏:
    TRADEDATE (–¥–∞—Ç–∞ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ YYYY-MM-DD.md),
    MD5_hash (md5 —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞),
    VECTORS (—ç–º–±–µ–¥–¥–∏–Ω–≥ —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ OllamaEmbeddingFunction).
    """
    # –°–æ–∑–¥–∞—ë–º lookup –ø–æ TRADEDATE –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫—ç—à–∞
    cache_lookup = {}
    if existing_df is not None and not existing_df.empty:
        cache_lookup = {
            row["TRADEDATE"]: {
                "MD5_hash": row["MD5_hash"],
                "VECTORS": row["VECTORS"],
            }
            for _, row in existing_df.iterrows()
        }

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª–æ–≤–∞—Ä—å, —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å TRADEDATE
    result_dict = {}  # TRADEDATE -> {"MD5_hash": ..., "VECTORS": ...}

    md_files = sorted(md_dir.glob("*.md"))
    logging.info(f"–ù–∞–π–¥–µ–Ω–æ markdown-—Ñ–∞–π–ª–æ–≤: {len(md_files)}")

    for md_file in md_files:
        try:
            tradedate_str = md_file.stem  # 'YYYY-MM-DD'
        except Exception as e:
            logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞—Ç—É –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ {md_file.name}: {e}")
            continue

        try:
            text = md_file.read_text(encoding='utf-8')
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {md_file}: {e}")
            continue

        if not text.strip():
            logging.info(f"–ü—É—Å—Ç–æ–π —Ñ–∞–π–ª, –ø—Ä–æ–ø—É—Å–∫: {md_file}")
            continue

        # MD5-—Ö—ç—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        md5_hash = hashlib.md5(text.encode('utf-8')).hexdigest()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ —Ñ–∞–π–ª
        cached = cache_lookup.get(tradedate_str)

        if cached and cached["MD5_hash"] == md5_hash:
            # –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π ‚Äî –±–µ—Ä—ë–º –∏–∑ –∫—ç—à–∞
            result_dict[tradedate_str] = {
                "TRADEDATE": tradedate_str,
                "MD5_hash": md5_hash,
                "VECTORS": cached["VECTORS"],
            }
            logging.info(f"{md_file.name}: –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –≤–∑—è—Ç–æ –∏–∑ –∫—ç—à–∞")
            continue

        # === –§–∞–π–ª –∏–∑–º–µ–Ω–∏–ª—Å—è –∏–ª–∏ –µ–≥–æ –Ω–µ –±—ã–ª–æ ‚Äî –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º ===
        # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∫–∞–∫ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏)
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        chunks = []
        current_chunk = []
        current_len = 0

        for para in paragraphs:
            para_len = token_len(para)
            if current_len + para_len > max_chunk_tokens and current_chunk:
                chunks.append('\n\n'.join(current_chunk))
                current_chunk = [para]
                current_len = para_len
            else:
                current_chunk.append(para)
                current_len += para_len
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))

        # === –ó–ê–©–ò–¢–ê –û–¢ –ü–£–°–¢–´–• –ß–ê–ù–ö–û–í ===
        # chunks = [c for c in chunks if c.strip()]
        if not chunks:
            logging.warning(f"{md_file.name}: –≤—Å–µ —á–∞–Ω–∫–∏ –ø—É—Å—Ç—ã–µ, —Ñ–∞–π–ª –ø—Ä–æ–ø—É—â–µ–Ω")
            continue

        total_tokens = sum(token_len(p) for p in paragraphs)
        logging.info(f"{md_file.name}: —á–∞–Ω–∫–æ–≤={len(chunks)}, —Ç–æ–∫–µ–Ω–æ–≤={total_tokens}")

        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–∞–Ω–∫–æ–≤
        chunk_embeddings = []
        for chunk in chunks:
            try:
                emb = ef([chunk])[0]
                chunk_embeddings.append(emb)
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ —á–∞–Ω–∫–∞ –≤ {md_file}: {e}")

        if not chunk_embeddings:
            continue

        # === –ü–†–û–í–ï–†–ö–ê –†–ê–ó–ú–ï–†–ù–û–°–¢–ò ===
        dims = {len(e) for e in chunk_embeddings}
        if len(dims) != 1:
            logging.error(f"–ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ {md_file.name}: {dims}")
            continue

        # === –£–°–†–ï–î–ù–ï–ù–ò–ï ===
        embedding = np.mean(chunk_embeddings, axis=0)  #.tolist()

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∏–ª–∏ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∑–∞–ø–∏—Å—å –ø–æ –¥–∞—Ç–µ
        result_dict[tradedate_str] = {
            "TRADEDATE": tradedate_str,
            "MD5_hash": md5_hash,
            "VECTORS": embedding,
        }

    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –∏–∑ —Å–ª–æ–≤–∞—Ä—è ‚Äî –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ –¥–∞—Ç–µ
    df = pd.DataFrame(list(result_dict.values()), columns=["TRADEDATE", "MD5_hash", "VECTORS"])
    df = df.sort_values("TRADEDATE").reset_index(drop=True)  # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
    logging.info(f"–°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, —Å—Ç—Ä–æ–∫: {len(df)}")
    return df

if __name__ == "__main__":
    existing_df = load_existing_cache(cache_file)

    df_embeddings = build_embeddings_df(md_path, existing_df)

    print(len(df_embeddings))

    with pd.option_context(
        "display.width", 1000,
        "display.max_columns", 10,
        "display.max_colwidth", 120
    ):
        print("–î–∞—Ç–∞—Ñ—Ä–µ–π–º —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏:")
        print(df_embeddings.head())

    print(len(df_embeddings['VECTORS'].iloc[0]))

    try:
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        with open(cache_file, 'wb') as f:
            pickle.dump(df_embeddings, f)
        logging.info(f"–ö—ç—à –æ–±–Ω–æ–≤–ª—ë–Ω –≤ {cache_file}, –≤—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df_embeddings)}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫—ç—à–∞ –≤ {cache_file}: {str(e)}")
