"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫—ç—à–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ markdown-—Ñ–∞–π–ª–æ–≤.
–ö—ç—à–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ pickle-—Ñ–∞–π–ª, –æ–±–Ω–æ–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ/–∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã.
"""

from pathlib import Path
import pickle
import hashlib
import numpy as np
from langchain_core.documents import Document
from chromadb.utils.embedding_functions import OllamaEmbeddingFunction
import logging
import yaml
from datetime import datetime

# –ü—É—Ç—å –∫ settings.yaml –≤ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, —á—Ç–æ –∏ —Å–∫—Ä–∏–ø—Ç
SETTINGS_FILE = Path(__file__).parent / "settings.yaml"

# –ß—Ç–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫
with open(SETTINGS_FILE, 'r', encoding='utf-8') as f:
    settings = yaml.safe_load(f)

# ==== –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ====
ticker = settings['ticker']
ticker_lc = ticker.lower()
url_ai = settings.get('url_ai', 'http://localhost:11434/api/embeddings')  # Ollama API –±–µ–∑ —Ç–∞–π–º-–∞—É—Ç–∞
model_name = settings.get('model_name', 'bge-m3')  # Ollama –º–æ–¥–µ–ª—å
md_path = Path(settings['md_path'])  # –ü—É—Ç—å –∫ markdown-—Ñ–∞–π–ª–∞–º

# –ü—É—Ç—å –∫ pkl-—Ñ–∞–π–ª—É —Å –∫—ç—à–µ–º
cache_file = Path(settings['cache_file'].replace('{ticker_lc}', ticker_lc))

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è –ª–æ–≥–æ–≤
log_dir = Path(__file__).parent / 'log'
log_dir.mkdir(parents=True, exist_ok=True)

# –ò–º—è —Ñ–∞–π–ª–∞ –ª–æ–≥–∞ —Å –¥–∞—Ç–æ–π –∏ –≤—Ä–µ–º–µ–Ω–µ–º –∑–∞–ø—É—Å–∫–∞ (–æ–¥–∏–Ω —Ñ–∞–π–ª –Ω–∞ –∑–∞–ø—É—Å–∫!)
timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
log_file = log_dir / f'create_embedding_ollama_{timestamp}.txt'

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: –¢–û–õ–¨–ö–û –æ–¥–∏–Ω —Ñ–∞–π–ª + –∫–æ–Ω—Å–æ–ª—å
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),  # –æ–¥–∏–Ω —Ñ–∞–π–ª
        logging.StreamHandler()                           # –∫–æ–Ω—Å–æ–ª—å
    ]
)

# –†—É—á–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤ (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ 3 —Å–∞–º—ã—Ö –Ω–æ–≤—ã—Ö)
def cleanup_old_logs(log_dir: Path, max_files: int = 3):
    """–£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –ª–æ–≥-—Ñ–∞–π–ª—ã, –æ—Å—Ç–∞–≤–ª—è—è max_files —Å–∞–º—ã—Ö –Ω–æ–≤—ã—Ö."""
    log_files = sorted(log_dir.glob("create_embedding_ollama_*.txt"))
    if len(log_files) > max_files:
        for old_file in log_files[:-max_files]:
            try:
                old_file.unlink()
                print(f"–£–¥–∞–ª—ë–Ω —Å—Ç–∞—Ä—ã–π –ª–æ–≥: {old_file.name}")
            except Exception as e:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {old_file}: {e}")

# –í—ã–∑—ã–≤–∞–µ–º –æ—á–∏—Å—Ç–∫—É –ü–ï–†–ï–î –Ω–∞—á–∞–ª–æ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
cleanup_old_logs(log_dir, max_files=3)
logging.info(f"üöÄ –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞. –õ–æ–≥-—Ñ–∞–π–ª: {log_file}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏ —Å markdown —Ñ–∞–π–ª–∞–º–∏
    if not md_path.exists():
        logging.error(f"–ü–∞–ø–∫–∞ —Å markdown —Ñ–∞–π–ª–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {md_path}")
        return

    # –ó–∞–≥—Ä—É–∑–∫–∞ markdown-—Ñ–∞–π–ª–æ–≤
    documents = load_markdown_files(md_path)
    if not documents:
        logging.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å markdown —Ñ–∞–π–ª—ã")
        return

    # –°–æ–∑–¥–∞–Ω–∏–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    cache = cache_embeddings(documents, cache_file, model_name, url_ai)
    logging.info("–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")


if __name__ == '__main__':
    main()
