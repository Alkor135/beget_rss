"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫—ç—à–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ markdown-—Ñ–∞–π–ª–æ–≤.
–ö—ç—à–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ pickle-—Ñ–∞–π–ª, –æ–±–Ω–æ–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ/–∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã.
"""

from pathlib import Path
import pickle
import hashlib
import numpy as np
from chromadb.utils.embedding_functions import OllamaEmbeddingFunction
import logging
import yaml
from datetime import datetime
import pandas as pd
import tiktoken

# –ü—É—Ç—å –∫ settings.yaml –≤ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, —á—Ç–æ –∏ —Å–∫—Ä–∏–ø—Ç
SETTINGS_FILE = Path(__file__).parent / "settings.yaml"

# –ß—Ç–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫
with open(SETTINGS_FILE, 'r', encoding='utf-8') as f:
    settings = yaml.safe_load(f)

# ==== –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ====
ticker = settings['ticker']
ticker_lc = ticker.lower()
url_ai = settings.get('url_ai', 'http://localhost:11434/api/embeddings')  # Ollama API –±–µ–∑ —Ç–∞–π–º-–∞—É—Ç–∞
model_name = settings.get('model_name', 'bge-m3')  # Ollama –º–æ–¥–µ–ª—å
md_path = Path(settings['md_path'])  # –ü—É—Ç—å –∫ markdown-—Ñ–∞–π–ª–∞–º

# –ü—É—Ç—å –∫ pkl-—Ñ–∞–π–ª—É —Å –∫—ç—à–µ–º
cache_file = Path(settings['cache_file'].replace('{ticker_lc}', ticker_lc))

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è –ª–æ–≥–æ–≤
log_dir = Path(__file__).parent / 'log'
log_dir.mkdir(parents=True, exist_ok=True)

# –ò–º—è —Ñ–∞–π–ª–∞ –ª–æ–≥–∞ —Å –¥–∞—Ç–æ–π –∏ –≤—Ä–µ–º–µ–Ω–µ–º –∑–∞–ø—É—Å–∫–∞ (–æ–¥–∏–Ω —Ñ–∞–π–ª –Ω–∞ –∑–∞–ø—É—Å–∫!)
timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
log_file = log_dir / f'create_embedding_ollama_{timestamp}.txt'

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: –¢–û–õ–¨–ö–û –æ–¥–∏–Ω —Ñ–∞–π–ª + –∫–æ–Ω—Å–æ–ª—å
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),  # –æ–¥–∏–Ω —Ñ–∞–π–ª
        logging.StreamHandler()                           # –∫–æ–Ω—Å–æ–ª—å
    ]
)

# –†—É—á–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤ (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ 3 —Å–∞–º—ã—Ö –Ω–æ–≤—ã—Ö)
def cleanup_old_logs(log_dir: Path, max_files: int = 3):
    """–£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –ª–æ–≥-—Ñ–∞–π–ª—ã, –æ—Å—Ç–∞–≤–ª—è—è max_files —Å–∞–º—ã—Ö –Ω–æ–≤—ã—Ö."""
    log_files = sorted(log_dir.glob("create_embedding_ollama_*.txt"))
    if len(log_files) > max_files:
        for old_file in log_files[:-max_files]:
            try:
                old_file.unlink()
                print(f"–£–¥–∞–ª—ë–Ω —Å—Ç–∞—Ä—ã–π –ª–æ–≥: {old_file.name}")
            except Exception as e:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {old_file}: {e}")

# –í—ã–∑—ã–≤–∞–µ–º –æ—á–∏—Å—Ç–∫—É –ü–ï–†–ï–î –Ω–∞—á–∞–ª–æ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
cleanup_old_logs(log_dir, max_files=3)
logging.info(f"üöÄ –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞. –õ–æ–≥-—Ñ–∞–π–ª: {log_file}")

enc = tiktoken.get_encoding("cl100k_base")

def token_len(text: str) -> int:
    return len(enc.encode(text))

# === –§—É–Ω–∫—Ü–∏—è –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ Ollama ===
ef = OllamaEmbeddingFunction(model_name=model_name)

def md5_of_file(path: Path) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç MD5-—Ö—ç—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞."""
    with open(path, 'r', encoding='utf-8') as file:
        content = file.read()
    md5_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
    return md5_hash

def load_existing_cache(cache_file: Path) -> pd.DataFrame | None:
    if cache_file.exists():
        try:
            with open(cache_file, "rb") as f:
                df = pickle.load(f)
            logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫—ç—à: {cache_file}, —Å—Ç—Ä–æ–∫: {len(df)}")
            return df
        except Exception as e:
            logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à {cache_file}: {e}")
    return None

def build_embeddings_df(md_dir: Path, existing_df: pd.DataFrame | None) -> pd.DataFrame:
    """
    –°–æ–∑–¥–∞—ë—Ç –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏:
    TRADEDATE (–¥–∞—Ç–∞ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ YYYY-MM-DD.md),
    MD5_hash (md5 —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞),
    VECTORS (—ç–º–±–µ–¥–¥–∏–Ω–≥ —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ OllamaEmbeddingFunction).
    """
    cache_lookup = {}

    if existing_df is not None and not existing_df.empty:
        cache_lookup = {
            row["TRADEDATE"]: {
                "MD5_hash": row["MD5_hash"],
                "VECTORS": row["VECTORS"],
            }
            for _, row in existing_df.iterrows()
        }

    records = []

    md_files = sorted(md_dir.glob("*.md"))
    logging.info(f"–ù–∞–π–¥–µ–Ω–æ markdown-—Ñ–∞–π–ª–æ–≤: {len(md_files)}")

    for md_file in md_files:
        # –ò–º—è —Ñ–∞–π–ª–∞ –æ–∂–∏–¥–∞–µ—Ç—Å—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD.md
        try:
            tradedate_str = md_file.stem  # 'YYYY-MM-DD'
        except Exception as e:
            logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞—Ç—É –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ {md_file.name}: {e}")
            continue

        try:
            text = md_file.read_text(encoding='utf-8')
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {md_file}: {e}")
            continue

        if not text.strip():
            logging.info(f"–ü—É—Å—Ç–æ–π —Ñ–∞–π–ª, –ø—Ä–æ–ø—É—Å–∫: {md_file}")
            continue

        # MD5-—Ö—ç—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        md5_hash = md5_of_file(md_file)

        cached = cache_lookup.get(tradedate_str)

        if cached and cached["MD5_hash"] == md5_hash:
            # === –§–ê–ô–õ –ù–ï –ò–ó–ú–ï–ù–ò–õ–°–Ø ===
            records.append(
                {
                    "TRADEDATE": tradedate_str,
                    "MD5_hash": md5_hash,
                    "VECTORS": cached["VECTORS"],
                }
            )
            logging.info(f"{md_file.name}: –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –≤–∑—è—Ç–æ –∏–∑ –∫—ç—à–∞")
            continue

        # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∫–∞–∫ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏)
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        chunks = []
        current_chunk = []
        current_len = 0

        if model_name == 'bge-m3':
            max_chunk_tokens = 7000  # –î–ª—è bge-m3 (8192 –ª–∏–º–∏—Ç –º–∏–Ω—É—Å –∑–∞–ø–∞—Å)
        elif model_name == 'qwen3-embedding:0.6b':
            max_chunk_tokens = 30000  # –î–ª—è qwen3-embedding:0.6b (32768 –ª–∏–º–∏—Ç –º–∏–Ω—É—Å –∑–∞–ø–∞—Å)
        else:
            print('–ü—Ä–æ–≤–µ—Ä—å –º–æ–¥–µ–ª—å')

        for para in paragraphs:
            para_len = token_len(para)  # –ü—Ä–∏–º–µ—Ä–Ω–æ —Ç–æ–∫–µ–Ω—ã
            if current_len + para_len > max_chunk_tokens and current_chunk:
                chunks.append('\n\n'.join(current_chunk))
                current_chunk = [para]
                current_len = para_len
            else:
                current_chunk.append(para)
                current_len += para_len
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))

        logging.info(f"{md_file.name}: —á–∞–Ω–∫–æ–≤={len(chunks)}, —Ç–æ–∫–µ–Ω–æ–≤~={current_len}")

        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–∞–Ω–∫–æ–≤
        chunk_embeddings = []
        for chunk in chunks:
            try:
                emb = ef([chunk])[0]
                chunk_embeddings.append(emb)
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ —á–∞–Ω–∫–∞ –≤ {md_file}: {e}")

        if not chunk_embeddings:
            continue

        # === –ü–†–û–í–ï–†–ö–ê –†–ê–ó–ú–ï–†–ù–û–°–¢–ò ===
        dims = {len(e) for e in chunk_embeddings}
        if len(dims) != 1:
            logging.error(
                f"–ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ {md_file.name}: {dims}"
            )
            continue

        # === –£–°–†–ï–î–ù–ï–ù–ò–ï ===
        embedding = np.mean(chunk_embeddings, axis=0).tolist()

        records.append(
            {
                "TRADEDATE": tradedate_str,
                "MD5_hash": md5_hash,
                "VECTORS": embedding,
            }
        )

    df = pd.DataFrame(records, columns=["TRADEDATE", "MD5_hash", "VECTORS"])
    logging.info(f"–°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, —Å—Ç—Ä–æ–∫: {len(df)}")
    return df

if __name__ == "__main__":
    existing_df = load_existing_cache(cache_file)

    df_embeddings = build_embeddings_df(md_path, existing_df)

    print(len(df_embeddings))

    with pd.option_context(
        "display.width", 1000,
        "display.max_columns", 10,
        "display.max_colwidth", 120
    ):
        print("–î–∞—Ç–∞—Ñ—Ä–µ–π–º —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏:")
        print(df_embeddings.head())
    print(len(df_embeddings['VECTORS'].iloc[0]))

    try:
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        with open(cache_file, 'wb') as f:
            pickle.dump(df_embeddings, f)
        logging.info(f"–ö—ç—à –æ–±–Ω–æ–≤–ª—ë–Ω –≤ {cache_file}, –≤—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df_embeddings)}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫—ç—à–∞ –≤ {cache_file}: {str(e)}")
