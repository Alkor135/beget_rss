"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è backtests —Å —Ä–∞–∑–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ max_prev_files –æ—Ç 4 –¥–æ 30.
–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ –¥–∞—Ç—ã –∏ cumulative_next_bar_pips –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –æ–¥–∏–Ω XLSX —Ñ–∞–π–ª –Ω–∞ –æ–¥–∏–Ω –ª–∏—Å—Ç.
–ö–æ–ª–æ–Ω–∫–∏: test_date, max_4, max_5, ..., max_30.

‚ö°Ô∏è–ò–∑–º–µ–Ω–µ–Ω–æ:
- –£–±—Ä–∞–Ω–∞ –ª–æ–≥–∏–∫–∞ –ø–µ—Ä–µ—Å—á—ë—Ç–∞ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è pkl (–∫—ç—à –¥–æ–ª–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤–æ–≤–∞—Ç—å –∑–∞—Ä–∞–Ω–µ–µ).
- –¢–µ–ø–µ—Ä—å –∫—ç—à —Ç–æ–ª—å–∫–æ —á–∏—Ç–∞–µ—Ç—Å—è. –ï—Å–ª–∏ pkl –Ω–µ—Ç ‚Üí –æ—à–∏–±–∫–∞.
- –£—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è, —á—Ç–æ –≤ –∫—ç—à–µ id = md5(page_content).
"""

import pandas as pd
from pathlib import Path
import pickle
import hashlib
import numpy as np
import sqlite3
from langchain_core.documents import Document
import logging
import yaml

# –ü—É—Ç—å –∫ settings.yaml –≤ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, —á—Ç–æ –∏ —Å–∫—Ä–∏–ø—Ç
SETTINGS_FILE = Path(__file__).parent / "settings.yaml"

# –ß—Ç–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫
with open(SETTINGS_FILE, 'r', encoding='utf-8') as f:
    settings = yaml.safe_load(f)

# ==== –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ====
ticker = settings.get('ticker', "RTS")  # –¢–∏–∫–µ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
ticker_lc = ticker.lower()
provider = settings.get('provider', 'investing')  # –ü—Ä–æ–≤–∞–π–¥–µ—Ä RSS –Ω–æ–≤–æ—Å—Ç–µ–π
min_prev_files = settings.get('min_prev_files', 2)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ñ–∞–π–ª–æ–≤
test_days = settings.get('test_days', None)  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è —Ç–µ—Å—Ç–∞ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å)
end_date_limit = settings.get('end_date_limit', None)  # –Ω–∞–ø—Ä–∏–º–µ—Ä '2025-09-30'

md_path = Path(  # –ü—É—Ç—å –∫ markdown-—Ñ–∞–π–ª–∞–º
    settings['md_path'].replace('{ticker_lc}', ticker_lc).replace('{provider}', provider))
cache_file = Path(  # –ü—É—Ç—å –∫ –∫—ç—à-—Ñ–∞–π–ª—É —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    settings['cache_file'].replace('{ticker_lc}', ticker_lc).replace('{provider}', provider))
path_db_day = Path(settings['path_db_day'].replace('{ticker}', ticker))
output_dir = Path(  # –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    settings['output_dir'].replace('{ticker_lc}', ticker_lc).replace('{provider}', provider))
log_file = Path(  # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –ª–æ–≥–∞
    output_dir / 'log' / # –ü–∞–ø–∫–∞ –¥–ª—è –ª–æ–≥–æ–≤
    fr'{ticker_lc}_backtest_multi_max_{provider}_{end_date_limit}.txt')  # –§–∞–π–ª –ª–æ–≥–∞
output_file = Path(  # –ò—Ç–æ–≥–æ–≤—ã–π XLSX —Ñ–∞–π–ª
    fr'C:\Users\Alkor\PycharmProjects\beget_rss\{ticker_lc}'
    fr'\{ticker_lc}_backtest_results_multi_max_{provider}_{end_date_limit}.xlsx')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
log_file.parent.mkdir(parents=True, exist_ok=True)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logger.handlers = []
console_handler = logging.StreamHandler()
console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger.addHandler(console_handler)
file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger.addHandler(file_handler)


def cosine_similarity(vec1, vec2):
    """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏."""
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return dot_product / (norm1 * norm2)

def load_markdown_files(directory):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ MD-—Ñ–∞–π–ª—ã –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç –ø–æ –¥–∞—Ç–µ (–∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞)."""
    files = sorted(directory.glob("*.md"), key=lambda f: f.stem)
    documents = []
    for file_path in files:
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()

        # # ‚ö° –î–æ–±–∞–≤–ª—è–µ–º md5 –æ—Ç –≤—Å–µ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞ (YAML + —Ç–µ–∫—Å—Ç)
        md5_hash = hashlib.md5(content.encode('utf-8')).hexdigest()  # –Ω–æ–≤–æ–≥–æ md5 —Ä–∞–∑–∫–æ–º–º–µ–Ω—Ç–∏—Ç—å

        if content.startswith('---'):
            parts = content.split('---', 2)
            if len(parts) >= 3:
                metadata_yaml = parts[1].strip()
                text_content = parts[2].strip()
                metadata = yaml.safe_load(metadata_yaml) or {}
                metadata_str = {
                    "next_bar": str(metadata.get("next_bar", "unknown")),
                    "date_min": str(metadata.get("date_min", "unknown")),
                    "date_max": str(metadata.get("date_max", "unknown")),
                    "source": file_path.name,
                    "date": file_path.stem,
                    "md5": md5_hash  # ‚ö° —Å–æ—Ö—Ä–∞–Ω—è–µ–º md5 –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                }
                doc = Document(page_content=text_content, metadata=metadata_str)
                documents.append(doc)
    return documents

def load_quotes(path_db_quote):
    """–ß–∏—Ç–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É Futures –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∫–æ—Ç–∏—Ä–æ–≤–æ–∫ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame —Å next_bar_pips."""
    with sqlite3.connect(path_db_quote) as conn:
        df = pd.read_sql_query("SELECT TRADEDATE, OPEN, CLOSE FROM Futures", conn)
    df = df.sort_values('TRADEDATE', ascending=True)
    df['TRADEDATE'] = df['TRADEDATE'].astype(str)
    df['next_bar_pips'] = df.apply(lambda x: (x['CLOSE'] - x['OPEN']), axis=1).shift(-1)
    df = df.dropna(subset=['next_bar_pips'])
    return df[['TRADEDATE', 'next_bar_pips']].set_index('TRADEDATE')


def load_cache(cache_file):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≥–æ—Ç–æ–≤—ã–π –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
    ‚ö°Ô∏è–ò–∑–º–µ–Ω–µ–Ω–æ: –∑–¥–µ—Å—å –±–æ–ª—å—à–µ –Ω–µ—Ç –ø–µ—Ä–µ—Å—á—ë—Ç–∞, —Ç–æ–ª—å–∫–æ —á—Ç–µ–Ω–∏–µ.
    """
    if not cache_file.exists():
        logger.error(f"–û—à–∏–±–∫–∞: –∫—ç—à {cache_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π –µ–≥–æ –¥—Ä—É–≥–∏–º —Å–∫—Ä–∏–ø—Ç–æ–º.")
        exit(1)

    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ {cache_file}")
    with open(cache_file, 'rb') as f:
        cache = pickle.load(f)
    return cache


def backtest_predictions(documents, cache, quotes_df, max_prev_files):
    """–ü—Ä–æ–≤–æ–¥–∏—Ç backtesting –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ max_prev_files –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame —Å test_date –∏ cumulative_next_bar_pips."""
    results = []

    # ‚ûï –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–Ω–µ–π
    if test_days:
        start_idx = max(min_prev_files, len(documents) - test_days)
    else:
        start_idx = min_prev_files

    for test_idx in range(start_idx, len(documents)):  # for test_idx in range(min_prev_files, len(documents)):
        test_doc = documents[test_idx]
        real_next_bar = test_doc.metadata['next_bar']
        test_date = test_doc.metadata['date']

        if real_next_bar == 'unknown' or real_next_bar == 'None':
            continue

        # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –¥–∞—Ç—ã –∏–∑ –∫—ç—à–∞ (–ø–æ md5)
        test_id = test_doc.metadata.get("md5")  # ‚ö° —Ç–µ–ø–µ—Ä—å –±–µ—Ä—ë–º md5 –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        test_embedding = next((item['embedding'] for item in cache if item['id'] == test_id), None)
        if test_embedding is None:
            continue

        # –ü—Ä–µ–¥—ã–¥—É—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –∫—ç—à–∞
        prev_cache = sorted(
            [item for item in cache if item['metadata']['date'] < test_date],
            key=lambda x: x['metadata']['date'], reverse=True
        )[:max_prev_files]

        if len(prev_cache) < min_prev_files:
            continue

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤
        similarities = [
            (cosine_similarity(test_embedding, item['embedding']) * 100, item['metadata'])
            for item in prev_cache
        ]
        similarities.sort(key=lambda x: x[0], reverse=True)

        # –ë–ª–∏–∂–∞–π—à–∏–π –¥–æ–∫—É–º–µ–Ω—Ç
        if similarities:
            closest_similarity, closest_metadata = similarities[0]
            predicted_next_bar = closest_metadata['next_bar']
            is_correct = predicted_next_bar == real_next_bar

            try:
                next_bar_pips_value = quotes_df.loc[test_date, 'next_bar_pips']
                next_bar_pips = abs(next_bar_pips_value) if is_correct else -abs(next_bar_pips_value)
            except KeyError:
                continue

            results.append({
                'test_date': test_date,
                'next_bar_pips': next_bar_pips
            })

    results_df = pd.DataFrame(results)
    if not results_df.empty:
        results_df['cumulative_next_bar_pips'] = results_df['next_bar_pips'].cumsum()
        return results_df[['test_date', 'cumulative_next_bar_pips']]
    else:
        return pd.DataFrame()


def main():
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ç–∏—Ä–æ–≤–æ–∫
    if not path_db_day.exists():
        logger.error(f"–û—à–∏–±–∫–∞: –§–∞–π–ª –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∫–æ—Ç–∏—Ä–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω: {path_db_day}")
        exit(1)
    quotes_df = load_quotes(path_db_day)

    # –ó–∞–≥—Ä—É–∑–∫–∞ markdown-—Ñ–∞–π–ª–æ–≤
    documents = load_markdown_files(md_path)
    if len(documents) < min_prev_files + 1:
        logger.error(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(documents)}. –¢—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º {min_prev_files + 1}.")
        exit(1)

    # ‚ö°Ô∏è–ó–∞–≥—Ä—É–∑–∫–∞ –≥–æ—Ç–æ–≤–æ–≥–æ –∫—ç—à–∞ (–±–µ–∑ –ø–µ—Ä–µ—Å—á—ë—Ç–∞)
    cache = load_cache(cache_file)

    # –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ DataFrame
    all_results = pd.DataFrame()

    # ‚ö°Ô∏è –ï—Å–ª–∏ –∑–∞–¥–∞–Ω–∞ –∫–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ ‚Äî –æ–±—Ä–µ–∂–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –∫–æ—Ç–∏—Ä–æ–≤–∫–∏
    if end_date_limit:
        documents = [doc for doc in documents if doc.metadata['date'] <= end_date_limit]
        quotes_df = quotes_df.loc[quotes_df.index <= end_date_limit]
        logger.info(
            f"–î–∞–Ω–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã –ø–æ –¥–∞—Ç—É: {end_date_limit}. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")

    for max_prev in range(3, 31):  # –æ—Ç 3 –¥–æ 30
        logger.info(f"–ü—Ä–æ–≤–æ–¥–∏–º backtest –¥–ª—è max_prev_files md —Ñ–∞–π–ª–æ–≤ = {max_prev}")
        results_df = backtest_predictions(documents, cache, quotes_df, max_prev)
        if not results_df.empty:
            results_df = results_df.rename(columns={'cumulative_next_bar_pips': f'max_{max_prev}'})
            if all_results.empty:
                all_results = results_df
            else:
                all_results = all_results.merge(results_df, on='test_date', how='outer')

    # === –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–∞–∫—Å–∏–º—É–º–µ –≤ –∫–æ–Ω—Å–æ–ª—å –∏ –ª–æ–≥===
    if not all_results.empty:
        # –ù–∞–π–¥—ë–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É –ø–æ –¥–∞—Ç–µ
        last_row = all_results.sort_values('test_date').iloc[-1]

        # –ù–∞–π–¥—ë–º –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å 'max_'
        max_cols = [col for col in all_results.columns if col.startswith('max_')]

        # –°—Ä–µ–¥–∏ –Ω–∏—Ö ‚Äî –∫–æ–ª–æ–Ω–∫—É —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º
        col_with_max = last_row[max_cols].idxmax()
        max_value = last_row[col_with_max]

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–∫–∏ ('max_17' ‚Üí 17)
        max_prev_value = int(col_with_max.split('_')[1])

        # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–æ–∫–∏
        last_date = last_row['test_date']

        # –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å –∏ –ª–æ–≥
        logger.info(f"üìÖ –ü–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞: {last_date}")
        logger.info(f"üèÜ –ö–æ–ª–æ–Ω–∫–∞ —Å –º–∞–∫—Å–∏–º—É–º–æ–º: {col_with_max} (max_prev_files = {max_prev_value})")
        logger.info(f"üìà –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {max_value:.2f}")

    # === –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Excel —Ñ–∞–π–ª ===
    if not all_results.empty:
        all_results.to_excel(output_file, index=False, engine='openpyxl')
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")

    else:
        logger.error("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.")


if __name__ == '__main__':
    main()