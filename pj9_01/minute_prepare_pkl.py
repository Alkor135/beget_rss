#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# minute_prepare_pkl.py
"""
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–∏–Ω—É—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ —Ñ—å—é—á–µ—Ä—Å—É RTS –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö SQLite-–±–∞–∑ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç
—Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ PKL. –ü–æ–ª–µ SECID –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏—Å–∫–ª—é—á–µ–Ω–æ. –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Å—Ç—Ä–æ–∫ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç—Å—è
–∫–ª—é—á–æ–º TRADEDATE (–≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –ë–î –ø–æ–ª–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ).

–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è:
- H2 ‚Äî –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã —á–µ—Ä–µ–∑ 2 —á–∞—Å–∞ –ø–æ—Å–ª–µ –æ—Ç–∫—Ä—ã—Ç–∏—è;
- H2_abs ‚Äî –∞–±—Å–æ–ª—é—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ H2;
- Percentile ‚Äî –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å H2_abs –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö LOOKBACK_DAYS —Ç–æ—Ä–≥–æ–≤—ã—Ö –¥–Ω–µ–π.

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: —á–∏—Ç–∞–µ—Ç—Å—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π PKL, –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ TRADEDATE;
- Percentile –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è –∑–∞–ø–∏—Å–µ–π –±–µ–∑ –∑–Ω–∞—á–µ–Ω–∏—è, —Å —É–∑–∫–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.
"""

import sqlite3
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
from tqdm import tqdm
import glob
import logging
import yaml

# =======================
# –ù–ê–°–¢–†–û–ô–ö–ò –ò –õ–û–ì–ò
# =======================

SETTINGS_FILE = Path(__file__).parent / "settings_rts.yaml"
with open(SETTINGS_FILE, "r", encoding="utf-8") as f:
    settings = yaml.safe_load(f)

TICKER = settings.get("ticker", "RTS")
SOURCE_DIR = Path(settings.get("path_db_dir", ""))              # –ü–∞–ø–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ SQLite
SOURCE_MASK = settings["path_db_min_file"].replace("{ticker}", TICKER)  # –ú–∞—Å–∫–∞ —Ñ–∞–π–ª–æ–≤
LOOKBACK_DAYS = int(settings.get("lookback_days", 10))

TARGET_PKL = f"minutes_{TICKER}_processed_p{LOOKBACK_DAYS}.pkl"
LOG_FILE = "minute_prepare.log"

logger = logging.getLogger("minute_prepare_pkl")
logger.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")

ch = logging.StreamHandler()
ch.setLevel(logging.INFO)
ch.setFormatter(formatter)
logger.addHandler(ch)

fh = logging.FileHandler(LOG_FILE, mode="a", encoding="utf-8")
fh.setLevel(logging.INFO)
fh.setFormatter(formatter)
logger.addHandler(fh)

# =======================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï
# =======================

def ensure_datetime(df: pd.DataFrame, cols):
    for c in cols:
        if c in df.columns and not pd.api.types.is_datetime64_any_dtype(df[c]):
            df[c] = pd.to_datetime(df[c], errors="coerce")
    return df

def process_H2(df: pd.DataFrame) -> pd.DataFrame:
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç H2 –∏ H2_abs –ø–æ –º–∏–Ω—É—Ç–Ω—ã–º –¥–∞–Ω–Ω—ã–º.
    –¢—Ä–µ–±—É–µ–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã: TRADEDATE, OPEN, CLOSE.
    """
    df = df.sort_values("TRADEDATE").reset_index(drop=True)
    df["TRADEDATE_2h"] = df["TRADEDATE"] + timedelta(hours=2)

    # –ë—É–¥—É—â–∏–µ CLOSE —á–µ—Ä–µ–∑ 2 —á–∞—Å–∞ –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é –≤—Ä–µ–º–µ–Ω–∏
    future = df[["TRADEDATE", "CLOSE"]].rename(
        columns={"TRADEDATE": "TRADEDATE_future", "CLOSE": "CLOSE_future"}
    )
    df = df.merge(
        future,
        left_on="TRADEDATE_2h",
        right_on="TRADEDATE_future",
        how="left",
    )

    df["H2"] = df["CLOSE_future"] - df["OPEN"]
    df["H2_abs"] = df["H2"].abs()
    df.drop(columns=["TRADEDATE_2h", "TRADEDATE_future", "CLOSE_future"], inplace=True)
    return df

def compute_percentile(df: pd.DataFrame, lookback_days: int) -> pd.DataFrame:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç Percentile –¥–ª—è H2_abs –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö lookback_days —Ç–æ—Ä–≥–æ–≤—ã—Ö –¥–Ω–µ–π.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç df —Å –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–º —Å—Ç–æ–ª–±—Ü–æ–º Percentile (–≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0,1]).
    """
    df = df.sort_values("TRADEDATE").reset_index(drop=True)
    if "Percentile" not in df.columns:
        df["Percentile"] = pd.NA

    df["__DATE__"] = df["TRADEDATE"].dt.date
    unique_dates = df["__DATE__"].drop_duplicates().tolist()
    idx_by_date = {d: i for i, d in enumerate(unique_dates)}

    pbar = tqdm(total=len(df), desc="Percentile", leave=False)
    vals = []
    for _, row in df.iterrows():
        d = row["__DATE__"]
        cur = idx_by_date[d]
        start = max(0, cur - lookback_days)
        back_dates = unique_dates[start:cur]
        if not back_dates or pd.isna(row["H2_abs"]):
            vals.append(pd.NA)
        else:
            pool = df[df["__DATE__"].isin(back_dates)]["H2_abs"].dropna()
            vals.append((pool <= row["H2_abs"]).sum() / len(pool) if len(pool) else pd.NA)
        pbar.update(1)
    pbar.close()

    df["Percentile"] = vals
    df.drop(columns="__DATE__", inplace=True)
    return df

def load_existing() -> pd.DataFrame:
    p = Path(TARGET_PKL)
    if p.exists():
        df = pd.read_pickle(p)
        df = ensure_datetime(df, ["TRADEDATE", "LSTTRADE"])
        return df
    return pd.DataFrame()

def save_all(df: pd.DataFrame):
    df = df.sort_values("TRADEDATE").drop_duplicates(subset=["TRADEDATE"], keep="last")
    df = df.reset_index(drop=True)
    Path(TARGET_PKL).parent.mkdir(parents=True, exist_ok=True)
    df.to_pickle(TARGET_PKL)
    logger.info(f"üéØ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {TARGET_PKL}")

# =======================
# –û–°–ù–û–í–ù–û–ô –ü–†–û–¶–ï–°–°
# =======================

def main():
    start = datetime.now()
    logger.info("===== –ó–∞–ø—É—Å–∫ minute_prepare_pkl.py (–±–µ–∑ SECID) =====")

    df_all = load_existing()

    source_files = sorted(glob.glob(str(Path(SOURCE_DIR) / SOURCE_MASK)))
    if not source_files:
        logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã –ø–æ –º–∞—Å–∫–µ: {Path(SOURCE_DIR) / SOURCE_MASK}")

    for src_file in source_files:
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞–∑—ã: {src_file}")
        with sqlite3.connect(src_file) as conn_src:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –∏ –ë–ï–ó SECID
            df_src = pd.read_sql(
                "SELECT TRADEDATE, OPEN, LOW, HIGH, CLOSE, VOLUME, LSTTRADE FROM Futures",
                conn_src,
                parse_dates=["TRADEDATE", "LSTTRADE"],
            )

        df_src = ensure_datetime(df_src, ["TRADEDATE", "LSTTRADE"])

        # –§–∏–ª—å—Ç—Ä –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫: –≤ –∏—Å—Ö–æ–¥–Ω–∏–∫–µ TRADEDATE —É–Ω–∏–∫–∞–ª–µ–Ω, –ø–æ—ç—Ç–æ–º—É –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–ª–∏—á–∏—è
        if not df_all.empty:
            existing_dates = set(df_all["TRADEDATE"].astype("datetime64[ns]"))
            df_src = df_src[~df_src["TRADEDATE"].isin(existing_dates)]

        if df_src.empty:
            logger.info("–ù–µ—Ç –Ω–æ–≤—ã—Ö –±–∞—Ä–æ–≤ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            continue

        logger.info(f"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ H2/H2_abs –¥–ª—è {len(df_src)} –∑–∞–ø–∏—Å–µ–π")
        df_src = process_H2(df_src)

        if df_all.empty:
            df_all = df_src.copy()
        else:
            df_all = pd.concat([df_all, df_src], ignore_index=True)

        # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        save_all(df_all)

    if df_all.empty:
        logger.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ Percentile ‚Äî –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ.")
        logger.info("===== –ì–æ—Ç–æ–≤–æ (–ø—É—Å—Ç–æ) =====")
        return

    if "Percentile" not in df_all.columns:
        df_all["Percentile"] = pd.NA

    mask_new = df_all["Percentile"].isna()
    if not mask_new.any():
        logger.info("–ù–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –±–µ–∑ Percentile –Ω–µ—Ç ‚Äî –ø–µ—Ä–µ—Å—á—ë—Ç –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
        save_all(df_all)
        logger.info("===== –ì–æ—Ç–æ–≤–æ –±–µ–∑ –ø–µ—Ä–µ—Å—á—ë—Ç–∞ Percentile =====")
        return

    df_new = df_all[mask_new].copy()
    last_new_date = df_new["TRADEDATE"].max().date()
    min_ctx_date = last_new_date - timedelta(days=LOOKBACK_DAYS + 1)

    ctx = df_all[df_all["TRADEDATE"].dt.date <= last_new_date]
    ctx = ctx[ctx["TRADEDATE"].dt.date >= min_ctx_date]

    logger.info(
        f"–†–∞—Å—á—ë—Ç Percentile: –æ–∫–Ω–æ {LOOKBACK_DAYS} –¥–Ω–µ–π, –∫–æ–Ω—Ç–µ–∫—Å—Ç {min_ctx_date}..{last_new_date}"
    )

    ctx = compute_percentile(ctx, LOOKBACK_DAYS)

    upd = ctx[["TRADEDATE", "Percentile"]].dropna(subset=["Percentile"])
    df_all = df_all.merge(upd, on="TRADEDATE", how="left", suffixes=("", "_new"))
    need_fill = df_all["Percentile"].isna() & df_all["Percentile_new"].notna()
    df_all.loc[need_fill, "Percentile"] = df_all.loc[need_fill, "Percentile_new"]
    if "Percentile_new" in df_all.columns:
        df_all.drop(columns=["Percentile_new"], inplace=True)

    save_all(df_all)

    end = datetime.now()
    logger.info(f"===== –ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {end - start} =====")


if __name__ == "__main__":
    main()
